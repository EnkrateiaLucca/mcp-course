{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830ae794",
   "metadata": {},
   "source": [
    "# Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2299ffa",
   "metadata": {},
   "source": [
    "![](2026-01-21-14-17-46.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20633d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import json\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959bf202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextBlock(citations=None, text='[get_weather(location=\"San Francisco\")]', type='text')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"What's the weather in San Francisco?\"\n",
    "# this is a from scratch example showing how tool calling began in the early days!\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=256,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "         Imagine you are a weather specialist with access to a 'get_weather' function that works like this:\n",
    "         \n",
    "         get_weather(location: str) -> str. \n",
    "         \n",
    "         When you the user asks for weather info, you're output shouldl be just the tool call with the right arguments.\n",
    "         make sure the output tool call is revolved by square brackets like: [get_weather(location=\"San Francisco\")]\n",
    "         \n",
    "         Here is the input from the user:\n",
    "        {user_input}\n",
    "         \"\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e64cdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[get_weather(location=\"San Francisco\")]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc90a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call:  get_weather(location=\"San Francisco\")\n",
      "Executing tool call:  san francisco\n",
      "Tool call executed successfully\n"
     ]
    }
   ],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    # This is a mock function - in reality, you'd call a weather API\n",
    "    weather_data = {\n",
    "        \"san francisco\": \"sunny, 72째F\",\n",
    "        \"new york\": \"cloudy, 65째F\",\n",
    "        \"london\": \"rainy, 58째F\",\n",
    "        \"tokyo\": \"clear, 68째F\"\n",
    "    }\n",
    "    location_lower = location.lower()\n",
    "    print('Executing tool call: ', location_lower)\n",
    "    return weather_data.get(location_lower, f\"Weather data not available for {location}\")\n",
    "\n",
    "\n",
    "def execute_tool_call(tool_call: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a tool call.\n",
    "    \n",
    "    Args:\n",
    "        tool_call: The tool call to execute\n",
    "        \n",
    "    Returns:\n",
    "        The result of the tool call\n",
    "    \"\"\"\n",
    "    # Parse the tool call\n",
    "    \n",
    "    \n",
    "    # Call the function\n",
    "    # strip the square brackets\n",
    "    tool_call = tool_call.strip(\"[]\")\n",
    "    if \"get_weather\" in tool_call:\n",
    "        print('Executing tool call: ', tool_call)\n",
    "        exec(tool_call)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tool: {tool_call}\")\n",
    "    \n",
    "    return \"Tool call executed successfully\"\n",
    "\n",
    "def llm_call(user_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Call the LLM with the user input.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user input\n",
    "        \n",
    "    Returns:\n",
    "        The response from the LLM\n",
    "    \"\"\"\n",
    "    client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=256,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "         Imagine you are a weather specialist with access to a 'get_weather' function that works like this:\n",
    "         \n",
    "         get_weather(location: str) -> str. \n",
    "         \n",
    "         When you the user asks for weather info, you're output shouldl be just the tool call with the right arguments.\n",
    "         make sure the output tool call is revolved by square brackets like: [get_weather(location=\"San Francisco\")]\n",
    "         \n",
    "         Here is the input from the user:\n",
    "        {user_input}\n",
    "         \"\"\"}\n",
    "    ]\n",
    ")\n",
    "    \n",
    "    return response.content[0].text\n",
    "\n",
    "# Test the tool calling\n",
    "user_input = \"What's the weather in san francisco?\"\n",
    "tool_call = llm_call(user_input)\n",
    "result = execute_tool_call(tool_call)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bed12",
   "metadata": {},
   "source": [
    "# .. What could be a use case when we use multiple different LLMs from different vendors in a single Agent? why not using the same LLMs within a single Agent?\n",
    "\n",
    "\n",
    "MCP!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2512979",
   "metadata": {},
   "source": [
    "# How much more training can you do to a model like claude when it has already been pre trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd2f10",
   "metadata": {},
   "source": [
    "YOu can't unless there is an API for fine tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76817d",
   "metadata": {},
   "source": [
    "# AT: is claude-sonnet-4-5 model that you use is something available in public like ollama that we can run locally? or this kind of nice interaction only available in hosted model? I'm still hazy about how this LLM manages is knowledge base\n",
    "\n",
    "its a closed source model! You use it under an API!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf01bf",
   "metadata": {},
   "source": [
    "# so in general it looks like the messages parameter is used to help the LLM know which tool to use. It's more on defining in the messages when it should be used? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef38818",
   "metadata": {},
   "source": [
    "Messages parameter is to keep track of conversation history (user inputs, tool calls, tool outputs. etc....)\n",
    "\n",
    "We can customize the system message or the prompt to teach and tell the model about the tools, but usually that's already done behind the HOOD!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
